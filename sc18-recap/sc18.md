# SC18

<img src="images/sc18.svg" alt="sc18 logo" title="SC18 Dallas, CO"/>

Conference Recap

Levi Baber

http://rit.las.iastate.edu
---
layout: true
class: footer-logo
---
<img src="images/booths1.jpg" alt="photo of conf booths"/ style="width:50%;float:right">

## Conf Overview
* ~13K people
* Attended >[20 sessions] over 5 days
* Gave a [lightning talk] to Xsede Campus Champions

<img src="images/xsede_cc.jpg" alt="photo of cc meeting"/ style="width:30%;float:right;margin:5px">

<img src="images/cray1.jpg" alt="photo of cc meeting"/ style="width:15%;float:right;margin:5px">

### Themes & trends
* Data growth & storage
* Simplifying software installs
* Reproducibility
* Containers

<img src="images/quantum.jpg" alt="photo of cc meeting"/ style="width:20%;float:right;margin:5px">

### Highlights
* User support: More users are entering HPC, not as technically proficient - need to meet their needs
* Hardware: Continued emphasis on GPU, FPGA
* Hardware: Emerging recognition that memory bandwidth and IOPS are becoming more important than increased FLOPS
* Funding: Discussed models with other university research computing staff

[20 sessions]: https://github.com/baberlevi/sc18_notes
[lightning talk]: https://researchit.github.io/RIT-Presentations/sc18_cc_lightning_talk/

---

## Software packaging with Spack

We've been using [Spack] to build scientific software for about a year

Spack is growing quickly:
<img src="images/spack_contributions.png" alt="photo of cc meeting"/ style="width:40%;float:right;margin:5px">
Currently contains ~3K packages

ISU got some recoginition during a couple different BoF sessions:
<img src="images/gssi_bof.jpg" alt="photo of cc meeting"/ style="width:50%;float:left;margin:5px">

Please let me know if you're interested in helping to build packages for Spack (can be used on Linux or Mac)

[Spack]: https://spack.io

---

## User support and training

### Web interface to HPC servers:
Open OnDemand: http://openondemand.org/  
Maybe deploy on okd 

### Training:
https://hpc-carpentry.github.io/  
Seem open to PRs  
Can offer 'unofficial' training for now, and I am in the queue for official training  

https://ideas-productivity.org/events/hpc-best-practices-webinars/
DoE webinars for HPC SW Devs - open to anyone


### Knowledgebase / Q & A:
https://ask.cyberinfrastructure.org/

---
## User support and training (cont)

### Software consulting:
Feel like we could do more to help some of our 'power' users to better optimize compilation and/or actual code

* Intel tools, and AVX512: https://tinyurl.com/sc18avx512
* https://openspeedshop.org/ 

OpenSpeedShop is a project of https://www.krellinst.org/ (based in Ames ?!)

Better Scientific Software: https://bssw.io/

### Bigger projects
* Push users more to Xsede, lots of resources out there for free
* Fontera (new TACC machine), coming mid next year (LCF requires additional justification)
* Port code to GPUs (look into Nvidia training / GPU Hackathons)

---
## Software support

Nvidia has a new container thing:
https://github.com/NVIDIA/hpc-container-maker   

### Spack updates
* Environments have been merged, ready for us to try out
* Stacks coming soon (combo of a list of apps you want * each arch)
* Binary packages are coming, will be hosted on s3
* New concretizer that will more aggressively re-use deps is coming soon

---
## Sys Admin stuff

### ReFrame: Automated regression testing (daily & post upgrade):
https://github.com/eth-cscs/reframe  
python based, runs via jenkins  
makes sure programs still work as expected, and gives ability to look at perf changes over time as patches, etc are applied

### OOOPS (iops rationing)
[Optimal Overloaded IO Protection System]  
Developed by TACC, available on request:

### Storage system stats
https://github.com/nersc/pytokio

### Automated data placement
https://github.com/pradsubedi/Stacker

### Clusterworks: Inception
Ansiblized deployment of openhpc on bare metal or cloud  
https://github.com/clusterworks/inception  
Sounds interesting, but haven't looked at it much. Maybe something like this for virtual cluster deployment on openstack ?

[Optimal Overloaded IO Protection System]: https://www.xsede.org/documents/10165/1756826/OOOPS-SiLiu.pdf/67398bef-6483-4b17-af11-6303191bcf58

---
## Sys Admin stuff (cont)

### Kraken: distributed state engine
Not production ready, but something to keep an eye on  
https://github.com/hpc/kraken

### Compliance / Information Security 
Notre Dame has a NIST 800-171 compliant solution
Involves a circuitous path through AWS GovCloud and back to their on-site HPC

### XALT2: tracking exe & lib usage
Sort of like what we do w/ lmod tracking, but actual CPU usage per exe  
https://xalt.readthedocs.io/en/latest/
---

## Reproducibility

Feel like there are a lot of competing 'platforms' still and this is going to take a while to get right.

http://rescue-hpc.org/program2018.html

http://rescue-hpc.org/_resources/20181111-TMalik-SemanticContainersHPC-Rescue2018-v2_rescue-hpc-sc18-workshop.pdf

https://sciunit.run/

---
## Follow ups items to research more

* Open Storage network: https://www.openstoragenetwork.org/
    * ceph w/ geo-replication
    * eta end of 2019
* FIONA node: 
    * https://fasterdata.es.net/science-dmz/DTN/fiona-flash-i-o-network-appliance/
    * we have one, need to talk to NetCom about using it to move data to/from LSS
* Open Science Grid: https://opensciencegrid.org/
    * have looked at it before, but might be worth sending some folks this direction and putting some of our hardware on it

---
## Interesting stuff

Google brain using ML to put ML jobs on GPUs:
* https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/grappler
* https://openreview.net/pdf?id=Hkc-TeZ0W

Growing interest in opensource hardware:
* https://github.com/ROCmSoftwarePlatform/rocALUTION
* https://www.opencompute.org/projects/high-performance-computing
* https://riscv.org/
* http://ascslab.org/research/briscv/index.html

---
## Conf Opportunities

* https://github.com/HPCSYSPROS/Workshop18
    * Said not a lot of submissions - would likely be easy to get a talk accepted next year
* Money for conference attendance for Xsede CC has been included in Frontera project
* https://www.pearc19.pearc.org/
    * I'm working on a paper w/ JHU & GT
