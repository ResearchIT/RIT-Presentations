# SC17

<img src="images/sc17.png" alt="sc17 logo" title="SC17 Denver, CO"/>

Conference Recap

Levi Baber

http://rit.las.iastate.edu

---

# Themes / Trends / Buzzwords

* Containers
* Reproducibility
* Traditional HPC / BigData / DL convergence
* Interactivity / UX
* GPU / TPU processing
* Configuration Management / Systems monitoring
* Heterogeneous hardware
* ARM
* Exascale

???

Sessions I attended:

* 20171112: HUST-17: https://hust17.github.io/
* 20171112: Best Practices in HPC training: https://sites.google.com/a/lbl.gov/hpc-training-best-practices/workshops/sc17
* 20171113: HPC System Professionals Workshop: http://hpcsyspros.lsu.edu/
* 20171114: How Serious Are We About the Convergence Between HPC and Big Data?: https://sc17.supercomputing.org/presentation/?id=pan128&sess=sess256
* 20171114: Reproducibility and Uncertainty in High Performance Computing: http://sc17.supercomputing.org/presentation/?id=pan121&sess=sess254
* 20171114: Usability, Scalability and Productivity on Many-Core Processors: Intel Xeon Phi: http://sc17.supercomputing.org/presentation/?id=bof128&sess=sess368
* 20171115: Practical Reproducibility by Managing Experiments Like Software: https://sc17.supercomputing.org/presentation/?id=bof177&sess=sess346
* 20171115: Blurring the Lines High-End Computing and Data Science: http://sc17.supercomputing.org/presentation/?id=pan105&sess=sess247
* 20171116: Common Big Data Challenges in Bio, Geo, Climate, and Social Sciences: https://grid.cs.gsu.edu/~tcpp/SC2017/
* 20171116: BeeGFS - Architecture, Implementation Examples, and Future Development: https://www.beegfs.io/content/sc17-bof/

---

# Hardware

* Omnipath, Infiniband, Ethernet
  * Omnipath seems to be gaining
  * Still some debate abbout offload/onload
  * Low latency / HighBW Ethernet ok for small clusters (=<64 nodes)
* Water cooling likely required for HPC using latest gen procs
* ARM - ThunderX gaining ground. Equivalent benchmarks on many workloads
* Lots of GPU dense deployments for Deep Learning

<img src="images/boiling.jpg" alt="cpu boiling"/ width=50%>

???
Omnipath seems stable and growing. Don't see any issues with us continuing down that path.
Talked to Cisco engineer about low latency Ethernet. He's a major contributor to OpenMPI. Good for small clusters that might have mixed use (bursty registration example)
Lots of mixed workloads with GPU/TPU (tensor processing) and some FPGA.  Expect GPU needs to become higher, with denser configurations also leading to more heat load.
New processors expected to require water cooling for most HPC densities (particularly considering added load of GPUs).  Some interesting solutions to the potential for leaks, pressure monitoring, negative pressure, etc. http://www.chilldyne.com/liquid-cooling-technology/

---

# HPC / Big Data / Deep Learning Convergence

* 


---

# Storage

* Lots of discussion around small file issues
  * Some interesting use of loopback filesystem with containers to overcome that
* Lustre
  * [Small files on MDT] - new in 2.11
* [BeeGFS]
  * good small file performance
  * high dirops at large scale
  * scales in pairs, metadata and storage can both be scaled (data ~ raid10)
  * involved in European Exascale projects
  * future: caching, target pools, snapshots, etc.
  * Probably good for a scratch system. Doesn't seem ready for long term storage yet.
* Burst Buffer
  * SSD cache closer to the compute
  * Auto migrate data during prologue
* Hyperconverged compute / storage
  * Permanent v. Ephemeral (e.g. BeeOND)
* Policy Engines (metadata cache / insight):
  * [Robinhood] - open source, tight integration with Lustre
  * [Starfish] - commercial

[BeeGFS]: https://www.beegfs.io
[Robinhood]:https://github.com/cea-hpc/robinhood/wiki
[Starfish]: http://storageconference.us/2017/Presentations/Farmer.pdf
[Small files on MDT]: http://wiki.lustre.org/Data_on_MDT

???
The latest dev version of Lustre (2.11) contains a feature for storing data on the meta data target, to reduce trips to the OST's. http://wiki.lustre.org/Data_on_MDT
Seems to be growing interest in storage policy engines to help manage really large file systems. Robinhood seems to have some new interest, and startups like Starfish are offering commercial solutions.

---

# Reproducibility / Replicability

* I went to several sessions on this topic
* Tools like [Spack] and [Singularity] will make this easier for the researchers
* Post upgrade testing - saw several tools, none of which were perfect / production ready
  * Can we use Singularity or Spack with CI to do this testing more effectively?
  * Purdue plans to release their [Testpilot] tool as open source
* Some concern about cpu arch & compiler differences
* Most concern about software stack and access to data & methods
* [Open Science Framework] got some small mention, but more focus on [Popper]

[Spack]:http://spack.readthedocs.io
[Singularity]: http://singularity.lbl.gov/
[Testpilot]: https://dl.acm.org/citation.cfm?doid=3152493.3152555
[Open Science Framework]:https://osf.io/
[Popper]:http://falsifiable.us/

---

# Job Scheduling (SLURM)

* [DMTCP] Checkpointing / [Job Suspsension with SLURM]
* [RC 17.11.0-0rc3] supports federated clusters
* Licenses can easily be managed as trackable resources (TRES)
* TACC did some work on a tool to add checkpointing, not production ready: [ITALC]

[DMTCP]: http://dmtcp.sourceforge.net/
[Job Suspsension with SLURM]: https://slurm.schedmd.com/SLUG17/ciemat-cr.pdf
[RC 17.11.0-0rc3]: https://download.schedmd.com/slurm/slurm-17.11.0-0rc3.tar.bz2
[ITALC]:https://www.tacc.utexas.edu/research-development/tacc-projects/italc

???
Several people I talked to said they'd tried job suspension with BLCR and had issues, but that DMTCP works well.
Federated clusters is a new feature that allows jobs to be submitted to multiple clusters and they coordinate with one another as peers.

---
# Containers

* Lot of attention for ease of software install & replicability
* Docker, Shifter, and [Charliecloud], but Singularity seems to have the most momentum
* Portability for people moving from local compute to Xsede or other national resources is a big benefit

[Shifter]:https://github.com/NERSC/shifter
[Charliecloud]: https://github.com/hpc/charliecloud

???
Fun fact: Greg Kurtzer who created Singularity also started Warewulf and CentOS https://gmkurtzer.github.io/

---

# Interactivity / UX

## Interactivity
* Need for interactive jobs growing as data & visualization needs outgrow desktops
* Setting different expectations for cluster utilization (how much science got done vs. being 90% utilized)
* [MIT Lincoln Lab] runs independent servers for interactive use (much like BioCrunch, Speedy, etc.)

[MIT Lincoln Lab]: https://www.ll.mit.edu/about/about.html

## User Experience / Training / Onboarding
* General consensus that HPC needs to be easier for new users
* Some good ideas:
  * Paired up mentoring with experienced users
  * [Parallware] & [HPC Sonar] to help with code parallelization
* Developing a [HPC Carpentry] curriculum for new user training
  * Provide some incentive for completion (badges, higher queue priority, etc.)

[Parallware]: https://www.parallware.com/
[HPC Sonar]: http://www.hpcsonar.com/
[HPC Carpentry]:https://github.com/hpccarpentry

---

# Other Interesting Stuff:
 ---

# Open Problelms
---
# Where do we fit / Follow ups:

* Training
