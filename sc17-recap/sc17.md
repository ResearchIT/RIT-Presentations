# SC17
Nov 2017
Denver, CO

<img src="images/sc17.png" alt="sc17 logo"/>

Conference Recap

Levi Baber

http://rit.las.iastate.edu

The notes below represent my views and opinions and should not be considered to represent an institutional position.

---

# Themes / Trends / Buzzwords

* Containers
* Reproducibility
* Traditional HPC / BigData, DL convergence
* Interactivity / UX
* GPU / TPU processing
* Configuration Management / Systems monitoring
* Heterogeneous hardware
* ARM
* Exascale

---

# Hardware

* Omnipath, Infiniband, Ethernet
  * Omnipath seems to be gaining
  * Still some debate abbout offload/onload
  * Low latency / HighBW Ethernet ok for small clusters (=<64 nodes)
* Water cooling likely required for HPC using latest gen procs
* ARM - ThunderX gaining ground. Equivalent benchmarks on many workloads
* Lots of GPU dense deployments for Deep Learning

<img src="images/boiling.jpg" alt="cpu boiling"/ width=50%>

???
Omnipath seems stable and growing. Don't see any issues with us continuing down that path.
Talked to Cisco engineer about low latency Ethernet. He's a major contributor to OpenMPI. Good for small clusters that might have mixed use (bursty registration example)
Lots of mixed workloads with GPU/TPU (tensor processing) and some FPGA.  Expect GPU needs to become higher, with denser configurations also leading to more heat load.
New processors expected to require water cooling for most HPC densities (particularly considering added load of GPUs).  Some interesting solutions to the potential for leaks, pressure monitoring, negative pressure, etc. http://www.chilldyne.com/liquid-cooling-technology/

---

# Storage

* Lots of discussion around small file issues
  * Some interesting use of loopback filesystem with containers to overcome that
* Lustre
  * Small files on MDT - new in 2.11 http://wiki.lustre.org/Data_on_MDT
* BeeGFS
  * good small file performance
  * high dirops at large scale
  * scales in pairs, metadata and storage can both be scaled (data ~ raid10)
  * involved in European Exascale projects
  * future: caching, target pools, snapshots, etc.
  * Probably good for a scratch system. Doesn't seem ready for long term storage.
* Policy Engines:
  * Robinhood: https://github.com/cea-hpc/robinhood/wiki
  * Starfish: http://storageconference.us/2017/Presentations/Farmer.pdf

???
The latest dev version of Lustre (2.11) contains a feature for storing data on the meta data target, to reduce trips to the OST's. http://wiki.lustre.org/Data_on_MDT
Seems to be growing interest in storage policy engines to help manage really large file systems. Robinhood seems to have some new interest, and startups like Starfish are offering commercial solutions.

---

# Reproducibility / Replicability

* I went to several sessions on this topic
* Tools like Spack and Singularity will make this easier for the researchers
* Post upgrade testing - saw several tools, none of which were great. How can we use Singularity or Spack to do this testing ?
* Some concern about cpu arch & compiler differnces
* Most concern about software stack and access to data & methods

---

# Job Scheduling (SLURM)

* Checkpoint (Suspsension): http://dmtcp.sourceforge.net/
* RC 17.11.0-0rc3 supports federated clusters
* Licenses can easily be managed as trackable resources (TRES)

???
Several people I talked to said they'd tried job suspension with BLCR and had issues, but that DMTCP works well.
Federated clusters is a new feature that allows jobs to be submitted to multiple clusters and they coordinate with one another as peers.

---
# Containers

* Lot of attention for ease of software install & replicability
* Some discussion of Docker, Shifter, and Charliecloud, but Singularity seems to have the most momentum
* Portability for people moving from local compute to Xsede or other national resources is a big deal



???
Fun fact: Greg Kurtzer who created Singularity also started Warewulf and CentOS https://gmkurtzer.github.io/
